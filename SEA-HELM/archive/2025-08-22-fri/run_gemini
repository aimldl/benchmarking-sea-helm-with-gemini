#!/bin/bash
# run_gemini

OUTPUT_DIR="output_gemini"
#MODEL_NAME="vertex_ai/gemini-2.5-pro"
MODEL_NAME="vertex_ai/gemini-2.5-flash"
#MODEL_NAME="vertex_ai/gemini-2.5-flash-lite"

echo 'python seahelm_evaluation.py --tasks seahelm --output_dir $OUTPUT_DIR --model_type vllm --model_name $MODEL_NAME --model_args "dtype=bfloat16,enable_prefix_caching=True,tensor_parallel_size=1"'

python seahelm_evaluation.py --tasks seahelm --output_dir $OUTPUT_DIR --model_type litellm --model_name $MODEL_NAME --model_args "api_provider=ollama,base_url=http://localhost:11434"
