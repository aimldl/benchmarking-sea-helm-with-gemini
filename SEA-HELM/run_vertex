#!/bin/bash
# run

OUTPUT_DIR="output_test_option1"
#MODEL_NAME="meta-llama/Llama-3-2-3B-Instruct"

OUTPUT_DIR="output_vertex_ai_gemini_2_5_pro"
MODEL_NAME="vertex_ai/gemini-2.5-pro"

echo 'python seahelm_evaluation.py --tasks seahelm --output_dir $OUTPUT_DIR --model_type vllm --model_name $MODEL_NAME --model_args "dtype=bfloat16,enable_prefix_caching=True,tensor_parallel_size=1"'

python seahelm_evaluation.py --tasks seahelm --output_dir $OUTPUT_DIR --model_type litellm --model_name $MODEL_NAME --model_args "api_provider=ollama,base_url=http://localhost:11434"
